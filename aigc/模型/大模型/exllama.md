# exllama

一个高效的内存重写的HF transformers使用量化权重的llama实现。
一个独立的llama基于Python/C++/CUDA实现。使用4-bit GPTQ，在现在的GPUs上更快，内存更高效。

github: [https://github.com/turboderp/exllama](https://github.com/turboderp/exllama)